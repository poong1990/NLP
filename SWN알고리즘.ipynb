{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 가져오기\n",
    "df = pd.read_csv('cooking.csv', encoding = 'utf-8')\n",
    "# 빈칸 있는 행 제거\n",
    "df.dropna(inplace = True)\n",
    "# comment 에 있는 문자열 다 소문자로 변경\n",
    "df['comment'] = df['comment'].str.lower()\n",
    "# comment 에 있는 글들 리스트에 저장\n",
    "sentence = list(df['comment'])\n",
    "# 빈 데이터프레임 생성\n",
    "result = pd.DataFrame(columns = [\"comment\", \"n_p-n\", \"v_p-n\", \"r_p-n\", \"sa_p-n\"])\n",
    "# 불용어 사전\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# 문자열 하나씩 가져와서 점수 생성하기\n",
    "for i in range(len(sentence)) :\n",
    "\n",
    "    comment = sentence[i]\n",
    "    words = []\n",
    "    # 토큰화\n",
    "    sent = pos_tag(word_tokenize(comment))\n",
    "    # 명사, 부사, 형용사, 동사 중 하나이고, 불용어가 아니면서 2글자 이상의 단어들만 저장\n",
    "    for s in sent :\n",
    "        if s[1] in ('NN', 'VB', 'VBG', 'VERB', 'ADV', 'JJ', 'ADJ') and s[0] not in stop_words :\n",
    "            if len(s[0]) > 2 :\n",
    "                words.append((s[0]))\n",
    "    # 한 문장에 겹치는 단어 없게끔 저장\n",
    "    words = list(set(words))\n",
    "\n",
    "#     words = list(set(comment.split(' ')))\n",
    "    # 명사\n",
    "    n_pos_avg = []\n",
    "    n_neg_avg = []\n",
    "    # 동사\n",
    "    v_pos_avg = []\n",
    "    v_neg_avg = []\n",
    "    # 부사\n",
    "    r_pos_avg = []\n",
    "    r_neg_avg = []\n",
    "    # 형용사\n",
    "    sa_pos_avg = []\n",
    "    sa_neg_avg = []\n",
    "    \n",
    "    # 처리된 단어가 있는 문장만 실행\n",
    "    try :\n",
    "        for k in words :\n",
    "            word_kind = list(swn.senti_synsets(k))\n",
    "\n",
    "            # 명사\n",
    "            n_pos = []\n",
    "            n_neg = []\n",
    "            # 동사\n",
    "            v_pos = []\n",
    "            v_neg = []\n",
    "            # 부사\n",
    "            r_pos = []\n",
    "            r_neg = []\n",
    "            # 형용사\n",
    "            sa_pos = []\n",
    "            sa_neg = []\n",
    "            \n",
    "            # 점수 가져오기\n",
    "            for word in word_kind:\n",
    "\n",
    "                synset = swn.senti_synset(str(word)[1:-1].split(':')[0])\n",
    "                temp= str(synset)[1:-1].split()\n",
    "\n",
    "                if temp[1].split('=')[1] != '0.0' or temp[2].split('=')[1] != '0.0' :\n",
    "\n",
    "                    if temp[0][:-1].split('.')[1] == 'n' :\n",
    "                        n_pos.append(float(temp[1].split('=')[1]))\n",
    "                        n_neg.append(float(temp[2].split('=')[1]))\n",
    "                    elif temp[0][:-1].split('.')[1] == 'v' :\n",
    "                        v_pos.append(float(temp[1].split('=')[1]))\n",
    "                        v_neg.append(float(temp[2].split('=')[1]))\n",
    "                    elif temp[0][:-1].split('.')[1] == 'r' :\n",
    "                        r_pos.append(float(temp[1].split('=')[1]))\n",
    "                        r_neg.append(float(temp[2].split('=')[1]))\n",
    "                    else :\n",
    "                        sa_pos.append(float(temp[1].split('=')[1]))\n",
    "                        sa_neg.append(float(temp[2].split('=')[1]))\n",
    "\n",
    "            n_sum_p = 0\n",
    "            n_sum_n = 0\n",
    "            v_sum_p = 0\n",
    "            v_sum_n = 0\n",
    "            r_sum_p = 0\n",
    "            r_sum_n = 0 \n",
    "            sa_sum_p = 0\n",
    "            sa_sum_n = 0\n",
    "            \n",
    "            # 단어별로 품사구분하여 점수의 평균값 저장\n",
    "            if len(n_pos) != 0 and len(n_neg) != 0 :\n",
    "\n",
    "                for i in n_pos :\n",
    "                    n_sum_p += i\n",
    "                for i in n_neg :\n",
    "                    n_sum_n += i\n",
    "\n",
    "                n_pos_avg.append(round((n_sum_p / len(n_pos)), 3))\n",
    "                n_neg_avg.append(round((n_sum_n / len(n_neg)), 3))\n",
    "            elif len(n_pos) == 0 and len(n_neg) == 0:\n",
    "                n_pos_avg.append(0.000)\n",
    "                n_neg_avg.append(0.000)\n",
    "            if len(v_pos) != 0 and len(v_neg) != 0 :\n",
    "\n",
    "                for i in v_pos :\n",
    "                    v_sum_p += i\n",
    "                for i in v_neg :\n",
    "                    v_sum_n += i\n",
    "\n",
    "                v_pos_avg.append(round((v_sum_p / len(v_pos)), 3))\n",
    "                v_neg_avg.append(round((v_sum_n / len(v_neg)), 3))\n",
    "            elif len(v_pos) == 0 and len(v_neg) == 0:\n",
    "                v_pos_avg.append(0.000)\n",
    "                v_neg_avg.append(0.000)\n",
    "            if len(r_pos) != 0 and len(r_neg) != 0 :\n",
    "\n",
    "                for i in r_pos :\n",
    "                    r_sum_p += i\n",
    "                for i in r_neg :\n",
    "                    r_sum_n += i\n",
    "\n",
    "                r_pos_avg.append(round((r_sum_p / len(r_pos)), 3))\n",
    "                r_neg_avg.append(round((r_sum_n / len(r_neg)), 3))\n",
    "            elif len(r_pos) == 0 and len(r_neg) == 0:\n",
    "                r_pos_avg.append(0.000)\n",
    "                r_neg_avg.append(0.000)\n",
    "            if len(sa_pos) != 0 and len(sa_neg) != 0 :\n",
    "\n",
    "                for i in sa_pos :\n",
    "                    sa_sum_p += i\n",
    "                for i in sa_neg :\n",
    "                    sa_sum_n += i\n",
    "\n",
    "                sa_pos_avg.append(round((sa_sum_p / len(sa_pos)), 3))\n",
    "                sa_neg_avg.append(round((sa_sum_n / len(sa_neg)), 3))\n",
    "            elif len(sa_pos) == 0 and len(sa_neg) == 0:\n",
    "                sa_pos_avg.append(0.000)\n",
    "                sa_neg_avg.append(0.000)\n",
    "\n",
    "        result=result.append({'comment' : comment, \n",
    "                              'n_p-n' :  max(n_pos_avg) - max(n_neg_avg), \n",
    "                              'v_p-n' : max(v_pos_avg) - max(v_neg_avg),\n",
    "                              'r_p-n' : max(r_pos_avg) - max(r_neg_avg),\n",
    "                              'sa_p-n' : max(sa_pos_avg) - max(sa_neg_avg),\n",
    "                             }, ignore_index = True)\n",
    "    except :\n",
    "        continue\n",
    "\n",
    "result.to_csv('anal_cooking.csv', encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
