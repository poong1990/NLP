{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 필수요소 패키지 설치\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize  \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### 파일 불러오기#########################################\n",
    "# 트레인 데이터\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "print(train_data)\n",
    "print('전체 리뷰 개수 :',len(train_data)) \n",
    "\n",
    "# reviews 에 있는 문자열 모두 소문자로 변경 (대문자가 있을 경우, 명사가 대명사로 인식되는 오류 발생)\n",
    "train_data['reviews'] = train_data['reviews'].str.lower()\n",
    "# 긍정 부정 점수 표기\n",
    "train_data['label'] = (train_data['label'] > 3) * 1\n",
    "# 이모티콘 제거\n",
    "tot_list = list(train_data['reviews'])\n",
    "new_list = []\n",
    "for i in tot_list :\n",
    "    new_list.append(re.sub(\"[^\\s!-~]+\", '', i))\n",
    "train_data['reviews'] = new_list\n",
    "# reviews 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data.drop_duplicates(subset=['reviews'], inplace=True)\n",
    "# 공백 제거\n",
    "train_data.dropna(inplace = True)\n",
    "print(train_data)\n",
    "print('전체 리뷰 개수 :',len(train_data))\n",
    "\n",
    "\n",
    "# 테스트 데이터\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "print(test_data)\n",
    "print('전체 리뷰 개수 :',len(test_data)) \n",
    "\n",
    "# reviews 에 있는 문자열 모두 소문자로 변경 (대문자가 있을 경우, 명사가 대명사로 인식되는 오류 발생)\n",
    "test_data['reviews'] = test_data['reviews'].str.lower()\n",
    "\n",
    "# 긍정 부정 점수 표기\n",
    "test_data['label'] = (test_data['label'] > 3) * 1\n",
    "# 이모티콘 제거\n",
    "tot_list = list(test_data['reviews'])\n",
    "new_list = []\n",
    "for i in tot_list :\n",
    "    new_list.append(re.sub(\"[^\\s!-~]+\", '', i))\n",
    "test_data['reviews'] = new_list\n",
    "# reviews 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data.drop_duplicates(subset=['reviews'], inplace=True)\n",
    "# 공백 제거\n",
    "test_data.dropna(inplace = True)\n",
    "print(test_data)\n",
    "print('전체 리뷰 개수 :',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### 데이터 토큰화 + 수치화 #########################################\n",
    "\n",
    "# 길이가 2이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "train_data['reviews'] = train_data['reviews'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "# token화 및 불용어 제거\n",
    "train_data['tokenized'] = train_data['reviews'].apply(word_tokenize)\n",
    "train_data['tokenized'] = train_data['tokenized'].apply(lambda x:[item for item in x if item not in stopwords.words('english')])\n",
    "\n",
    "# 긍정 부정 단어 구분\n",
    "negative_words = np.hstack(train_data[train_data.label == 0]['tokenized'].values)\n",
    "positive_words = np.hstack(train_data[train_data.label == 1]['tokenized'].values)\n",
    "\n",
    "# 정수 인코딩 단어에 번호를 붙이는 작업\n",
    "X_train = train_data['tokenized'].values\n",
    "y_train = train_data['label'].values \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 2                                 # 희귀단어 -> 2번이상 사용이 되지 않은 단어 \n",
    "total_cnt = len(tokenizer.word_index)         # 단어 개수\n",
    "rare_cnt = 0                                  # 등장 빈도가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0                                # 훈련데이터 전체 단어 빈도수 총합\n",
    "rare_freq = 0                                 # 등장 빈도수가 threshold보다 작은 단어의 빈도수의 총합\n",
    "\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 빈도수가 threshold보다 작을경우\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "# 전체 단어 개수 Total_cnt 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "\n",
    "# 정수 변환 완료\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'oov')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "def len_length (mex_len, overlap_list):\n",
    "    cnt = 0\n",
    "    for a in overlap_list:\n",
    "        if(len(a) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "\n",
    "max_len = 100\n",
    "len_length(max_len, X_train)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### 데이터 토큰화 + 수치화 #########################################\n",
    "\n",
    "# 길이가 2이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "test_data['reviews'] = test_data['reviews'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "# token화 및 불용어 제거\n",
    "test_data['tokenized'] = test_data['reviews'].apply(word_tokenize)\n",
    "test_data['tokenized'] = test_data['tokenized'].apply(lambda x:[item for item in x if item not in stopwords.words('english')])\n",
    "\n",
    "# 긍정 부정 단어 구분\n",
    "negative_words = np.hstack(test_data[test_data.label == 0]['tokenized'].values)\n",
    "positive_words = np.hstack(test_data[test_data.label == 1]['tokenized'].values)\n",
    "\n",
    "# 정수 인코딩 단어에 번호를 붙이는 작업\n",
    "X_test = test_data['tokenized'].values\n",
    "y_test = test_data['label'].values \n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "\n",
    "threshold = 2                                 # 희귀단어 -> 2번이상 사용이 되지 않은 단어 \n",
    "total_cnt = len(tokenizer.word_index)         # 단어 개수\n",
    "rare_cnt = 0                                  # 등장 빈도가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0                                # 훈련데이터 전체 단어 빈도수 총합\n",
    "rare_freq = 0                                 # 등장 빈도수가 threshold보다 작은 단어의 빈도수의 총합\n",
    "\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 빈도수가 threshold보다 작을경우\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "# 전체 단어 개수 Total_cnt 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "\n",
    "# 정수 변환 완료\n",
    "tokenizer = Tokenizer(vocab_size, oov_token = 'oov')\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "def len_length (mex_len, overlap_list):\n",
    "    cnt = 0\n",
    "    for a in overlap_list:\n",
    "        if(len(a) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "\n",
    "max_len = 100\n",
    "len_length(max_len, X_test)\n",
    "\n",
    "X_test = pad_sequences(X_test, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 머신러닝 돌리기 위해 실수화\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### 모델링 #########################################\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "# 단어집합의 크기, 100차원의 임베딩 벡터 표현\n",
    "model.add(Embedding(100000, 100)) \n",
    "model.add(GRU(256))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# EarlyStopping은 너무 많은 Epoch로 인한 Overfitting을 방지하기 위해 설정\n",
    "# es = Performance measure를 최소화 하고 verbose가 발생한 지점을 출력한다. \n",
    "# 증가하지 않는 epoch(patience)를 4번 허용한다.\n",
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 4) \n",
    "# ModelCheckpoint는 작업중 Callback을 할경우 최적의 모델을 선택 할수 있도록 한다.\n",
    "mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', made = 'max', \n",
    "                     verbose = 1, save_best_only = True)  \n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "hist = model.fit(X_train, y_train, epochs = 10, verbose = 1, callbacks = [es, mc], \n",
    "                 batch_size = 100, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 가져와서 예측\n",
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 함수 ###############################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize  \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def sentiment_predict(total_data) :\n",
    "    loaded_model = load_model('best_model.h5')   \n",
    "    total_data.dropna(inplace = True)\n",
    "    \n",
    "    # 길이가 2이하인 단어는 제거 (길이가 짧은 단어 제거)\n",
    "    total_data['comment'] = total_data['comment'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "    # token화 및 불용어 제거\n",
    "    total_data['tokenized'] = total_data['comment'].apply(word_tokenize)\n",
    "    total_data['tokenized'] = total_data['tokenized'].apply(lambda x:[item for item in x if item not in stopwords.words('english')])\n",
    "    X_train = total_data['tokenized'].values\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    threshold = 2                                 # 희귀단어 -> 2번이상 사용이 되지 않은 단어 \n",
    "    total_cnt = len(tokenizer.word_index)         # 단어 개수\n",
    "    rare_cnt = 0                                  # 등장 빈도가 threshold보다 작은 단어의 개수를 카운트\n",
    "    total_freq = 0                                # 훈련데이터 전체 단어 빈도수 총합\n",
    "    rare_freq = 0                                 # 등장 빈도수가 threshold보다 작은 단어의 빈도수의 총합\n",
    "\n",
    "    for key, value in tokenizer.word_counts.items():\n",
    "        total_freq = total_freq + value\n",
    "\n",
    "        # 단어의 빈도수가 threshold보다 작을경우\n",
    "        if(value < threshold):\n",
    "            rare_cnt = rare_cnt + 1\n",
    "            rare_freq = rare_freq + value\n",
    "\n",
    "    # 전체 단어 개수 Total_cnt 중 빈도수 2이하인 단어 개수는 제거.\n",
    "    # 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "    vocab_size = total_cnt - rare_cnt + 2\n",
    "\n",
    "    # 정수 변환 완료\n",
    "    tokenizer = Tokenizer(vocab_size, oov_token = 'oov')\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    # 채널별 나누기 (기준값은 host : 유튜브명)\n",
    "    host_li = list(set(total_data['host']))\n",
    "    video_li = []\n",
    "    for i in host_li :\n",
    "        video_li.append(list(set(total_data[total_data['host'] == i]['url'])))\n",
    "\n",
    "    p_pct = []\n",
    "    n_pct = []\n",
    "    host = []\n",
    "    video = []\n",
    "    sent_len = []\n",
    "\n",
    "    # All - All    \n",
    "    lis = list(total_data['comment'])\n",
    "    p_reviews_cnt = 0\n",
    "    n_reviews_cnt = 0\n",
    "\n",
    "    for i in lis :\n",
    "        new_sentence = i\n",
    "\n",
    "        new_sentence = word_tokenize(new_sentence)\n",
    "        # 불용어 제거\n",
    "        new_sentence = [word for word in new_sentence if not word in stopwords.words('english')] \n",
    "        # 정수 인코딩\n",
    "        encoded = tokenizer.texts_to_sequences([new_sentence]) \n",
    "        # 패딩\n",
    "        pad_new = pad_sequences(encoded, maxlen = 100) \n",
    "        # 예측\n",
    "        score = float(loaded_model.predict(pad_new)) \n",
    "\n",
    "        if(score > 0.5):\n",
    "            p_reviews_cnt += 1\n",
    "        else:\n",
    "            n_reviews_cnt += 1\n",
    "\n",
    "    p_pct.append(int(p_reviews_cnt / len(lis) * 100))\n",
    "    n_pct.append(int(n_reviews_cnt / len(lis) * 100))\n",
    "    host.append('All')\n",
    "    video.append('All')\n",
    "    sent_len.append(len(lis))\n",
    "\n",
    "\n",
    "    # Channel - All\n",
    "    for i in host_li :\n",
    "        lis = list(total_data[total_data['host'] == i]['comment'])\n",
    "        p_reviews_cnt = 0\n",
    "        n_reviews_cnt = 0\n",
    "\n",
    "        for j in range(len(lis)) :\n",
    "            new_sentence = lis[j]\n",
    "            new_sentence = word_tokenize(new_sentence)\n",
    "            # 불용어 제거\n",
    "            new_sentence = [word for word in new_sentence if not word in stopwords.words('english')] \n",
    "            # 정수 인코딩\n",
    "            encoded = tokenizer.texts_to_sequences([new_sentence]) \n",
    "            # 패딩\n",
    "            pad_new = pad_sequences(encoded, maxlen = 100) \n",
    "            # 예측\n",
    "            score = float(loaded_model.predict(pad_new)) \n",
    "\n",
    "            if(score > 0.5):\n",
    "                p_reviews_cnt += 1\n",
    "            else:\n",
    "                n_reviews_cnt += 1\n",
    "\n",
    "        p_pct.append(int(p_reviews_cnt / len(lis) * 100))\n",
    "        n_pct.append(int(n_reviews_cnt / len(lis) * 100))\n",
    "        host.append(i)\n",
    "        video.append('All')\n",
    "        sent_len.append(len(lis))\n",
    "\n",
    "    # Channel - Video\n",
    "    for k in video_li :\n",
    "        for m in range(len(k)) :\n",
    "            this_host = list(set(total_data[total_data['url'] == k[m]]['host']))\n",
    "            host.append(this_host[0])\n",
    "        for m in k :\n",
    "\n",
    "            lis = list(total_data[total_data['url'] == m]['comment'])\n",
    "            p_reviews_cnt = 0\n",
    "            n_reviews_cnt = 0\n",
    "            for n in range(len(lis)) :\n",
    "                new_sentence = lis[n]\n",
    "                new_sentence = word_tokenize(new_sentence)\n",
    "                # 불용어 제거\n",
    "                new_sentence = [word for word in new_sentence if not word in stopwords.words('english')] \n",
    "                # 정수 인코딩\n",
    "                encoded = tokenizer.texts_to_sequences([new_sentence]) \n",
    "                # 패딩\n",
    "                pad_new = pad_sequences(encoded, maxlen = 100) \n",
    "                # 예측\n",
    "                score = float(loaded_model.predict(pad_new)) \n",
    "\n",
    "                if(score > 0.5):\n",
    "                    p_reviews_cnt += 1\n",
    "                else:\n",
    "                    n_reviews_cnt += 1\n",
    "\n",
    "            p_pct.append(int(p_reviews_cnt / len(lis) * 100))\n",
    "            n_pct.append(100 - int(p_reviews_cnt / len(lis) * 100))\n",
    "            video.append(m)\n",
    "            sent_len.append(len(lis))\n",
    "\n",
    "    df = pd.DataFrame({'host' : host, \n",
    "                       'video' : video, \n",
    "                       'p_pct' : p_pct, \n",
    "                       'n_pct' : n_pct})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 유튜브 데이터로 예측 후 결과 csv저장\n",
    "baking = pd.read_csv('baking.csv')\n",
    "df = sentiment_predict(baking)\n",
    "df.to_csv('result_baking.csv', index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
